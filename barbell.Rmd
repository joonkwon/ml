---
title: "Unilateral Dumbbell Biceps Curl Classification"
author: "Joohyun Kwon"
date: "April 26, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Synopsis

Popularity of devices such as Jawbone Up, Nike FuelBand, and Fitbit makes the collection of personal activity data much easier than before. With these data many alaylisis have been carried out to identity what kind of and how much of activities people do in a certian period. However, not much analysis have been done on the quality of an activity. 
In our analysis, we try to classify a performance of Unilateral Dumbbell Biceps Curl to five different classes (class A to E). This is an attemp to quantity the quality of the excercise.

Given dataset are devided to traing and testing set. We train our algorithm with the training data. We used two algorithms and the result from both will be applied to the testing set for comparison. The algorithm with higher accuracy will be used to make the prediction. 

#Background

The dataset used here comes from Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Six young healthy participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

#Data Cleanup
Load the data.

```{r}
data.train = read.csv("pml-training.csv")
data.val = read.csv("pml-testing.csv")
```

data.train will be used for training and validation. data.val will be used for final submission. data.val doesn't have final classe labels.


Find zero Variance Column and remove that colums from data

```{r message=FALSE}
library(caret)
library(randomForest)
zeroVarCols = nearZeroVar(data.train)
data.sub = data.train[,-zeroVarCols]
```

We find NA columns and remove them. Also we remove the first 6 columns that shows timestamps, subject id, windows number. 

```{r}
naCols = colSums(is.na(data.sub)) > 10000
data = data.sub[,!naCols]
data = data[,7:59]
features = names(data)[-53]
data.val = data.val[,features]
```

#Split Data to Training and Testing

```{r}
idxTrain = createDataPartition(data$classe, p = 0.75, list=FALSE)
training = data[idxTrain,]
testing = data[-idxTrain,]
```

#Classification with Random Forest

First, we fit a model using Random Forest algorithm.

```{r cache=TRUE}
fit.rf = train(classe ~ ., data=training, method="rf",
               trControl=trainControl(method="cv",number=5), prox=TRUE,allowParallel=TRUE)
predict.rf = predict(fit.rf, testing)
confusionMatrix(predict.rf, testing$classe)
```

The classification using Random Forest algorithm shows over 99% accuracy.

#Classification with KNN

Next, we build a model using K-Nearest Neighbors. 

```{r cache=TRUE}
fit.knn = train(classe ~ ., data=training, method="knn", preProcess = c("center","scale"),
               trControl=trainControl(method="cv",number=5))

predict.knn = predict(fit.knn, testing)
confusionMatrix(predict.knn, testing$classe)
```

The accuracy of KNN algorithm is about 97%.


So, we decided to use the model with Random Forest for the prediction

#Prediction

```{r}
predict(fit.rf, data.val)
```
